{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab270a6d-a301-4f35-8951-f3cba56b6f1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Simple Streaming Listener\n",
    "# -------------------------------------------\n",
    "from pyspark.sql.streaming import StreamingQueryListener\n",
    "import logging, os, re\n",
    "from datetime import datetime\n",
    "\n",
    "def attach_streaming_listener(job_name: str, log_dir: str = \"/dbfs/tmp\"):\n",
    "    \"\"\"\n",
    "    Attach a lightweight listener that logs progress to a job-specific file.\n",
    "    No dependency on an external logger object.\n",
    "    \"\"\"\n",
    "    safe_job = re.sub(r\"[^\\w\\-]\", \"_\", job_name)\n",
    "    logger = logging.getLogger(f\"{safe_job}_listener\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        fh = logging.FileHandler(os.path.join(log_dir, f\"{safe_job}_listener_{ts}.log\"))\n",
    "        fh.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\"))\n",
    "        fh.setLevel(logging.INFO)\n",
    "        logger.addHandler(fh)\n",
    "        logger.propagate = False\n",
    "        logger.info(f\"{job_name} - Listener started.\")\n",
    "\n",
    "    class _Listener(StreamingQueryListener):\n",
    "        def onQueryStarted(self, event):\n",
    "            logger.info(f\"{job_name} - Query started: {event.name}\")\n",
    "\n",
    "        def onQueryProgress(self, event):\n",
    "            p = event.progress\n",
    "            src = p['sources'][0]['description'] if p['sources'] else \"Unknown\"\n",
    "            logger.info(f\"{job_name} - Batch {p['batchId']} at {p['timestamp']} - \"\n",
    "                        f\"inputRows={p['numInputRows']}, inputRPS={p['inputRowsPerSecond']}, \"\n",
    "                        f\"procRPS={p['processedRowsPerSecond']}, source={src}\")\n",
    "\n",
    "        def onQueryTerminated(self, event):\n",
    "            logger.info(f\"{job_name} - Query terminated.\")\n",
    "\n",
    "    spark.streams.addListener(_Listener())\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "10_streaming_listener",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
