{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4daff41f-f503-4ec4-b492-22fb6ae83241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Streaming Writer (Delta, availableNow)\n",
    "# -------------------------------------------\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import traceback\n",
    "\n",
    "def streaming_write_to_table(df, table_name: str, write_mode: str, job_name: str, source_path: str, query_name: str, logger):\n",
    "    try:\n",
    "        if not df.isStreaming:\n",
    "            logger.warning(f\"{job_name} - DataFrame is not streaming, skipping write.\")\n",
    "            return\n",
    "\n",
    "        checkpoint_path = f\"{source_path}/_checkpoints/{job_name}\"\n",
    "        output_path     = f\"{source_path}/_bronze_output/{job_name}\"\n",
    "\n",
    "        def log_batch(batch_df, batch_id):\n",
    "            try:\n",
    "                cnt = batch_df.count()\n",
    "                logger.info(f\"{job_name} - Batch {batch_id}: {cnt:,} rows\")\n",
    "            except Exception as be:\n",
    "                logger.warning(f\"{job_name} - Batch {batch_id}: log failed: {be}\")\n",
    "                logger.warning(traceback.format_exc())\n",
    "\n",
    "        q = (df.writeStream\n",
    "               .format(\"delta\")\n",
    "               .outputMode(write_mode)\n",
    "               .option(\"checkpointLocation\", checkpoint_path)\n",
    "               .queryName(query_name)                 # Enables StreamingQueryListener\n",
    "               .foreachBatch(log_batch)               # Logs row count\n",
    "               .trigger(availableNow=True)\n",
    "               .start(output_path))                   # Writes to path, not table directly\n",
    "               \n",
    "        logger.info(f\"{job_name} - writeStream started.\")\n",
    "        q.awaitTermination()\n",
    "        logger.info(f\"{job_name} - writeStream finished. Registering table if needed.\")\n",
    "\n",
    "        try:\n",
    "            spark.table(table_name)  # will succeed if exists\n",
    "            logger.info(f\"{job_name} - Table exists: {table_name}\")\n",
    "        except AnalysisException:\n",
    "            spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{output_path}'\")\n",
    "            logger.info(f\"{job_name} - Table registered: {table_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{job_name} - Streaming write failed: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "12_structured_streaming_file_write_incremental",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
