{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4daff41f-f503-4ec4-b492-22fb6ae83241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "import traceback\n",
    "\n",
    "# -----------------------------------------\n",
    "# Function to write Auto Loader streaming data to Delta table\n",
    "# -----------------------------------------\n",
    "def streaming_write_to_table(df, table_name, mode):\n",
    "    try:\n",
    "        print(f\"{JOB_NAME} - Starting streaming write to Delta table: {table_name}\")\n",
    "        logger.info(f\"{JOB_NAME} - Starting streaming write to Delta table: {table_name}\")\n",
    "\n",
    "        if df.isStreaming:\n",
    "            checkpoint_path = f\"{SOURCE_PATH}/_checkpoints/{JOB_NAME}\"\n",
    "\n",
    "            query = (\n",
    "                df.writeStream\n",
    "                  .format(\"delta\")\n",
    "                  .outputMode(\"append\")  # streaming doesn't support overwrite\n",
    "                  .option(\"checkpointLocation\", checkpoint_path)\n",
    "                  .trigger(once=True)  # batch-style incremental\n",
    "                  .toTable(table_name)\n",
    "            )\n",
    "\n",
    "            logger.info(f\"{JOB_NAME} - writeStream triggered successfully\")\n",
    "            query.awaitTermination()\n",
    "            logger.info(f\"{JOB_NAME} - writeStream completed for table: {table_name}\")\n",
    "\n",
    "        else:\n",
    "            # Optional fallback: non-streaming support\n",
    "            row_count = df.count()\n",
    "            if row_count == 0:\n",
    "                logger.warning(f\"{JOB_NAME} - No data to write to table: {table_name}\")\n",
    "                return\n",
    "\n",
    "            df.write.format(\"delta\") \\\n",
    "                .mode(mode) \\\n",
    "                .saveAsTable(table_name)\n",
    "\n",
    "            logger.info(f\"{JOB_NAME} - Write successful to table: {table_name}\")\n",
    "            logger.info(f\"{JOB_NAME} - Rows written: {row_count:,}\")\n",
    "\n",
    "    except AnalysisException as ae:\n",
    "        logger.error(f\"{JOB_NAME} - Table write failed due to analysis error\")\n",
    "        logger.error(f\"{JOB_NAME} - {str(ae)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise RuntimeError(f\"{JOB_NAME} - Analysis failure writing to {table_name}\") from ae\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{JOB_NAME} - General failure during write to table: {table_name}\")\n",
    "        logger.error(f\"{JOB_NAME} - Exception: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise RuntimeError(f\"{JOB_NAME} - Write failed to {table_name}\") from e"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "12_structured_streaming_file_write_incremental",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
