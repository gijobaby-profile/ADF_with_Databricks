{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a3895b0-21c9-4391-ae33-45215c092464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "import logging, traceback, uuid\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08dfd13-f95b-4390-940c-cebd84798e35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "AUDIT_CATALOG = \"adf_adb_audit\"\n",
    "AUDIT_SCHEMA  = \"audit\"\n",
    "AUDIT_TABLE   = \"pipeline_audit\"\n",
    "AUDIT_BASE_PATH=\"abfss://gizmoboxadb@gijodatabricksextdl.dfs.core.windows.net/adf_adb_audit/\"\n",
    "catalog_loc = AUDIT_BASE_PATH.rstrip(\"/\") + \"/\"\n",
    "schema_loc  = catalog_loc + \"audit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "447216a1-981f-4621-971d-c6ebd2c0e16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _now_utc():\n",
    "    return datetime.now(timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33224a8c-3a67-4233-9597-6f61e0a54edd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _dbutils_ctx():\n",
    "    out = {\"user\": None, \"triggered_by\": \"unknown\", \"job_id\": None,\n",
    "           \"run_id\": None, \"spark_version\": spark.version}\n",
    "    try:\n",
    "        from pyspark.dbutils import DBUtils\n",
    "        dbu = DBUtils(spark)  # type: ignore\n",
    "        nb = dbu.notebook.getContext()\n",
    "        try:\n",
    "            out[\"user\"] = spark.sql(\"select current_user()\").collect()[0][0]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            tags = {t.key(): t.value() for t in nb.tags().get()}\n",
    "            out[\"job_id\"] = tags.get(\"jobId\") or tags.get(\"job_id\")\n",
    "            out[\"run_id\"] = tags.get(\"jobRunId\") or tags.get(\"runId\")\n",
    "            if tags.get(\"jobId\"):\n",
    "                out[\"triggered_by\"] = \"schedule\"\n",
    "            else:\n",
    "                out[\"triggered_by\"] = \"manual\"\n",
    "        except:\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "    if not out[\"run_id\"]:\n",
    "        out[\"run_id\"] = str(uuid.uuid4())  # interactive fallback\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bff687ff-cee2-4a1a-972d-5f7a678fcffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE_CATALOG = f\"\"\"CREATE CATALOG IF NOT EXISTS {AUDIT_CATALOG}\n",
    "\t\t\t\t\t\tMANAGED LOCATION '{catalog_loc}'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0083d0ba-eb5a-4de0-be08-7a01569fab8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE_SCHEMA = f\"\"\"CREATE SCHEMA IF NOT EXISTS {AUDIT_SCHEMA}\n",
    "\t\t\t\t\tMANAGED LOCATION '{schema_loc}'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f27940f5-9425-4f7c-8c85-dc4698e18f46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE_TABLE = f\"\"\"CREATE TABLE IF NOT EXISTS {AUDIT_CATALOG}.{AUDIT_SCHEMA}.{AUDIT_TABLE} (\n",
    "\t\t\t\t\t\tenvironment STRING, \n",
    "\t\t\t\t\t\tjob_id STRING, \n",
    "\t\t\t\t\t\tjob_name STRING, \n",
    "\t\t\t\t\t\trun_id STRING,\n",
    "\t\t\t\t\t\ttriggered_by STRING, \n",
    "\t\t\t\t\t\ttarget_table STRING, \n",
    "\t\t\t\t\t\tlayer STRING,\n",
    "\t\t\t\t\t\tstart_time_utc TIMESTAMP, \n",
    "\t\t\t\t\t\tend_time_utc TIMESTAMP, \n",
    "\t\t\t\t\t\tduration_ms BIGINT,\n",
    "\t\t\t\t\t\trecord_count BIGINT, \n",
    "\t\t\t\t\t\trun_status STRING, \n",
    "\t\t\t\t\t\tretry_of_run_id STRING, \n",
    "\t\t\t\t\t\taudit_date DATE\n",
    "\t\t\t\t\t) USING DELTA PARTITIONED BY (audit_date)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ec74284-1c97-416b-8c79-f630c2f2c535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure audit catalog/schema/table exist with explicit storage locations.\n",
    "\n",
    "def ensure_audit_objects(logger=None,AUDIT_BASE_PATH=\"abfss://gizmoboxadb@gijodatabricksextdl.dfs.core.windows.net/adf_adb_audit/\"):\n",
    "    spark.sql(CREATE_CATALOG)\n",
    "    spark.sql(f\"USE CATALOG {AUDIT_CATALOG}\")\n",
    "    spark.sql(CREATE_SCHEMA)\n",
    "    spark.sql(CREATE_TABLE)\n",
    "\t\n",
    "    if logger: logger.info(\"Audit catalog/schema/table ensured.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34c30802-5752-44ce-8acc-a8ab21d83287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _layer_from_target_table(target_table: str) -> str:\n",
    "    # expects \"<catalog>.<schema>.<table>\"\n",
    "    parts = (target_table or \"\").split(\".\")\n",
    "    return parts[1] if len(parts) >= 2 else \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eb0f418-efe3-4a9b-9b08-88b43a9a2490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType, DateType\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "import uuid\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def audit_start(\n",
    "    pENV: str = None, ENV: str = None,\n",
    "    pJOB_NAME: str = None, JOB_NAME: str = None,\n",
    "    pTARGET_TABLE: str = None, TARGET_TABLE: str = None,\n",
    "    pRETRY_OF_RUN_ID: str = None, RETRY_OF_RUN_ID: str = None,\n",
    "    logger=None\n",
    ") -> dict:\n",
    "    \"\"\"Create/Upsert a RUNNING row for (run_id, layer) with an explicit schema.\"\"\"\n",
    "    ensure_audit_objects(logger)\n",
    "\n",
    "    # --- resolve args (support both corporate p* and plain names) ---\n",
    "    env          = (pENV or ENV or \"\").lower()\n",
    "    job_name     = pJOB_NAME or JOB_NAME\n",
    "    target_table = pTARGET_TABLE or TARGET_TABLE\n",
    "    retry_id     = pRETRY_OF_RUN_ID or RETRY_OF_RUN_ID\n",
    "\n",
    "    ctx = _dbutils_ctx()\n",
    "    run_id = ctx[\"run_id\"] or str(uuid.uuid4())\n",
    "    start_ts = datetime.now(timezone.utc)\n",
    "    layer = _layer_from_target_table(target_table)\n",
    "\n",
    "    # --- explicit schema to avoid NullType inference ---\n",
    "    schema = StructType([\n",
    "        StructField(\"environment\",    StringType(),   True),\n",
    "        StructField(\"job_id\",         StringType(),   True),\n",
    "        StructField(\"job_name\",       StringType(),   False),\n",
    "        StructField(\"run_id\",         StringType(),   False),\n",
    "        StructField(\"triggered_by\",   StringType(),   True),\n",
    "        StructField(\"target_table\",   StringType(),   True),\n",
    "        StructField(\"layer\",          StringType(),   True),\n",
    "        StructField(\"start_time_utc\", TimestampType(),True),\n",
    "        StructField(\"end_time_utc\",   TimestampType(),True),\n",
    "        StructField(\"duration_ms\",    LongType(),     True),\n",
    "        StructField(\"record_count\",   LongType(),     True),\n",
    "        StructField(\"run_status\",     StringType(),   True),\n",
    "        StructField(\"retry_of_run_id\",StringType(),   True),\n",
    "        StructField(\"audit_date\",     DateType(),     True)\n",
    "    ])\n",
    "\n",
    "    row = [{\n",
    "        \"environment\": env,\n",
    "        \"job_id\": ctx[\"job_id\"],\n",
    "        \"job_name\": job_name,\n",
    "        \"run_id\": run_id,\n",
    "        \"triggered_by\": ctx[\"triggered_by\"],\n",
    "        \"target_table\": target_table,\n",
    "        \"layer\": layer,\n",
    "        \"start_time_utc\": start_ts,\n",
    "        \"end_time_utc\": None,\n",
    "        \"duration_ms\": None,\n",
    "        \"record_count\": None,\n",
    "        \"run_status\": \"RUNNING\",\n",
    "        \"retry_of_run_id\": retry_id,\n",
    "        \"audit_date\": start_ts.date()\n",
    "    }]\n",
    "\n",
    "    df = spark.createDataFrame(row, schema=schema)\n",
    "\n",
    "    tgt = f\"{AUDIT_CATALOG}.{AUDIT_SCHEMA}.{AUDIT_TABLE}\"\n",
    "    DeltaTable.forName(spark, tgt).alias(\"t\") \\\n",
    "        .merge(df.alias(\"s\"), \"t.run_id = s.run_id AND t.layer = s.layer\") \\\n",
    "        .whenMatchedUpdateAll() \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()\n",
    "\n",
    "    if logger: logger.info(f\"[AUDIT] START {job_name} {layer} run_id={run_id}\")\n",
    "    return {\"run_id\": run_id, \"layer\": layer, \"start_time_utc\": start_ts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "744ea2e9-fc57-4d65-87a7-95852962d52a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def audit_update_count(*, RUN_ID: str, TARGET_TABLE: str,\n",
    "                       RECORD_COUNT: int, logger: logging.Logger = None):\n",
    "    layer = _layer_from_target_table(TARGET_TABLE)\n",
    "    tgt = DeltaTable.forName(spark, f\"{AUDIT_CATALOG}.{AUDIT_SCHEMA}.{AUDIT_TABLE}\")\n",
    "    upd = (spark.range(1)\n",
    "        .select(\n",
    "            F.lit(RUN_ID).alias(\"run_id\"),\n",
    "            F.lit(layer).alias(\"layer\"),\n",
    "            F.lit(int(RECORD_COUNT)).alias(\"record_count\")\n",
    "        ))\n",
    "    tgt.alias(\"t\").merge(\n",
    "        upd.alias(\"s\"),\n",
    "        \"t.run_id = s.run_id AND t.layer = s.layer\"\n",
    "    ).whenMatchedUpdate(set={\"record_count\": F.col(\"s.record_count\")}).execute()\n",
    "    if logger:\n",
    "        logger.info(f\"[AUDIT] COUNT {layer} rows={RECORD_COUNT} run_id={RUN_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64521878-64df-4a86-982c-a9dbf48326b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import timezone\n",
    "\n",
    "def audit_finalize(*, RUN_ID: str, TARGET_TABLE: str, RUN_STATUS: str, logger=None):\n",
    "    layer = _layer_from_target_table(TARGET_TABLE)\n",
    "    end_ts = _now_utc()  # aware UTC\n",
    "\n",
    "    tbl = f\"{AUDIT_CATALOG}.{AUDIT_SCHEMA}.{AUDIT_TABLE}\"\n",
    "    row = (spark.table(tbl)\n",
    "           .where((F.col(\"run_id\")==RUN_ID) & (F.col(\"layer\")==layer))\n",
    "           .select(\"start_time_utc\")\n",
    "           .limit(1).collect())\n",
    "\n",
    "    duration_ms = None\n",
    "    try:\n",
    "        st = row[0][0] if row else None\n",
    "        if st is not None:\n",
    "            # make start aware-UTC if Spark returned it naive\n",
    "            if getattr(st, \"tzinfo\", None) is None:\n",
    "                st = st.replace(tzinfo=timezone.utc)\n",
    "            else:\n",
    "                st = st.astimezone(timezone.utc)\n",
    "            duration_ms = int((end_ts - st).total_seconds() * 1000)\n",
    "    except Exception as ex:\n",
    "        if logger: logger.warning(f\"[AUDIT] duration calc failed: {ex}; continuing without duration.\")\n",
    "\n",
    "    upd = (spark.range(1).select(\n",
    "        F.lit(RUN_ID).alias(\"run_id\"),\n",
    "        F.lit(layer).alias(\"layer\"),\n",
    "        F.lit(RUN_STATUS).alias(\"run_status\"),\n",
    "        F.lit(end_ts).alias(\"end_time_utc\"),\n",
    "        F.lit(duration_ms).cast(\"bigint\").alias(\"duration_ms\"),\n",
    "        F.lit(end_ts.date()).alias(\"audit_date\")\n",
    "    ))\n",
    "    DeltaTable.forName(spark, tbl).alias(\"t\") \\\n",
    "      .merge(upd.alias(\"s\"), \"t.run_id = s.run_id AND t.layer = s.layer\") \\\n",
    "      .whenMatchedUpdate(set={\n",
    "          \"run_status\": F.col(\"s.run_status\"),\n",
    "          \"end_time_utc\": F.col(\"s.end_time_utc\"),\n",
    "          \"duration_ms\": F.col(\"s.duration_ms\"),\n",
    "          \"audit_date\": F.col(\"s.audit_date\"),\n",
    "      }).execute()\n",
    "\n",
    "    if logger: logger.info(f\"[AUDIT] FINAL {layer} status={RUN_STATUS} run_id={RUN_ID}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_audit_utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
